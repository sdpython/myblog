<?xml version="1.0" encoding="utf-8"?>
<html>
<head> 
<link TYPE="text/css" href="pMenu.css" rel="stylesheet"/>
<link type="text/css" rel="stylesheet" href="documents/graph_blog_20131130.css" />
<link type="text/css" rel="stylesheet" href="javascript/d3.js" />
<title>Intelligence artificielle : hope and despair</title>
<meta content="machine learning, intelligence artificielle" name="keywords" />
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
</head>
<body>

<!-- SUMMARY BEGINS -->

<p>
Le machine learning dans sa conception qui s'appuie sur des bases 
d'apprentissage et de test est une façon de synthétiser un savoir puis 
de le restituer. Ce savoir est une liste d'évènements passés, ce que l'on sait 
d'une manière plus ou moins structurée, il est restitué le plus souvent 
sous la forme d'une aide à la décision pour les décisions futures. 
Le machine learning ne fait rien d'autre que de dire, sur la base des 
éléments passés, qu'il se passerait probablement ceci pour ces 
nouveaux éléments. A vous de vous en servir au mieux.
</p>
<p>
Cela dit, cela dit, ... Je viens de voir l'épisode 7 de la saison 
7 de <a href="https://fr.wikipedia.org/wiki/The_Good_Wife">The Good Wife</a>
sur l'intelligence artificielle (<a href="https://fr.wikipedia.org/wiki/Saison_7_de_The_Good_Wife#%C3%89pisode_7_:_Intelligence_artificielle">S7E7</a>).
</p>

<!-- SUMMARY ENDS -->
<!-- CUT PAGE HERE -->

<p>
Cette série propose souvent de bons argumentaires sur les sujets techniques. 
Le problème posé de l'accident d'une voiture automatique. 
Un homme au volant, en fait occupé à faire autre chose, a heurté 
une autre voiture dont la conductrice s'est retrouvée paralysée des jambes. 
Le conducteur qui ne conduisait pas se retourne contre la société qui fabrique 
l'intelligence artificielle. Qui est responsable ? Je passe une partie 
sur la suffisance du concepteur de l'IA qui dénigre plus ou moins 
tout ce qui a un QI inférieur au sien, soit à peu près tout le monde 
d'après lui mais ce trait de caractère l'affuble d'une précision technique 
et de la recherche d'une explication obsessionnelle ce qui nous convient 
bien. Un argument contre le concepteur est issu d'un constat : une 
voiture programmée pour respecter scrupuleusement le code de la route 
est bloquée par d'autres conducteurs qui n'hésitent pas à freiner les autres 
voitures pour imposer la leur. Pour s'en sortir, l'intelligence artificielle est capable de 
prendre une décision dite agressive dans le seul but de ne pas faire du 
sur-place. Un autre argument vient du constant apprentissage de l'intelligence 
qui devient, en passant par la contraposée, que la voiture se sait pas conduire 
puisqu'elle apprend encore et toujours. Un peu comme nous d'ailleurs.
L'épisode se termine par une explication plus classique de hacking. 
La voiture aurait été pilotée par un hacker le temps de l'incident et il aurait pris 
soin d'effacer le disque dur de la voiture ce qui n'avait jamais été fait auparavant, 
d'où la suspicion. L'épisode est intéressant non pas pour sa conclusion mais 
pour la revue des causes probables de l'accident et leur interprétation au sens du 
droit. La série accentue sans doute un peu le trait. Il faut bien maintenir
le spectateur en action même si je préfèrerais un récit façon
<a href="https://fr.wikipedia.org/wiki/La_Controverse_de_Valladolid_(t%C3%A9l%C3%A9film)">Controverse de Valladolid</a>.
</p>

<p>
C'est pourquoi vous devriez continuer par la lecture du rapport de la CNIL :
<a href="https://www.cnil.fr/fr/comment-permettre-lhomme-de-garder-la-main-rapport-sur-les-enjeux-ethiques-des-algorithmes-et-de">Comment permettre à l’Homme de garder la main ?</a>.
Ca s'agite aussi de l'autre côté de l'océan
<a href="https://www.propublica.org/article/new-york-city-moves-to-create-accountability-for-algorithms">New York City Moves to Create Accountability for Algorithms</a>.
</p>
<p>
Je poursuis avec cette affirmation du concepteur de la machine, 
le gars suffisant de l'histoire, qui soutient, comme Bill Gates, Elon Musk 
rappelle-t-il, que l'intelligence artificielle représentera un grand danger 
pour l'humanité quand elle sera autonome. Même si cet objectif paraît 
encore loin, on sait construire des outils qui savent choisir la meilleure 
action parmi un ensemble déterminé à l'avance dans des contextes 
de plus en plus élaborés. Quant à avoir un algorithme qui imagine 
un truc nouveau, pour le moment je n'ai aucune raison de penser 
que cela va se produire bientôt. Selon la définition d'un algorithme, 
ça ne peut pas s'appeler comme tel. Cet épisode n'en est pas moins 
une petite introduction intéressante sur ce qu'est une intelligence artificielle.
</p>
<p>
Je me demande si nous ne sommes pas déjà capables d'imaginer une machine 
qui devienne autonome, nous ne sommes juste pas capable de faire en 
sorte qu'elle apprenne en un temps humain. Nous ne savons pas encore 
construire un assemblage de neurones artificiels en aussi grand nombre 
et aussi économes en énergie mais nous pouvons le concevoir. Alors nous produisons
des intelligences artificielles avec des objectifs précis que nous traduisons
de façon mathématique en un problème d'optimisation.
Et si un jour nous les programmons pour survivre...
<a href="https://www.wired.com/2016/05/google-alpha-go-ai/">What AI Behind Alpha-Go Can Teach Us About Being Human</a>.
Il reste que nous ne savons pas construire une intelligence artificielle qui consomme 
aussi peu que le cerveau humain. D'ailleurs, nous devrions peut-être ajouter une contrainte 
dans les affrontements hommes-machines : à énergie constante, qui est le meilleur ? 
</p>

</body>
</html>
