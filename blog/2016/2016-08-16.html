<?xml version="1.0" encoding="utf-8"?>
<html>
<head> 
<link TYPE="text/css" href="pMenu.css" rel="stylesheet"/>
<link type="text/css" rel="stylesheet" href="documents/graph_blog_20131130.css" />
<link type="text/css" rel="stylesheet" href="javascript/d3.js" />
<title>Reading, modules about machine learning...</title>
<meta content="python, biblio" name="keywords" />
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
</head>
<body>

<p>
How to train a model with imbalanced datasets (not enough
observation for a class), to do recommendations or to compute
confidence intervals on prediction with a random forest?
That's some of the answer the following extensions of 
scikit-learn try to answer.
<ul>
<li><a href="https://github.com/scikit-learn-contrib/imbalanced-learn">imbalanced-learn</a>
    imbalanced-learn is a python package offering a number of re-sampling techniques
    commonly used in datasets showing strong between-class imbalance. 
    It is compatible with scikit-learn and is part of scikit-learn-contrib projects.
    </li>
<li><a href="https://github.com/scikit-learn-contrib/polylearn">polylearn</a>
    Factorization machines and polynomial networks are machine learning models 
    that can capture feature interaction (co-occurrence) through polynomial terms. 
    Because feature interactions can be very sparse, 
    it's common to use low rank, factorized representations; this way, 
    we can learn weights even for feature co-occurrences that haven't 
    been observed at training time.
    </li>
<li><a href="https://github.com/scikit-learn-contrib/forest-confidence-interval">forest-confidence-interval</a>
    forest-confidence-interval is a Python module for calculating variance and adding 
    confidence intervals to scikit-learn random forest regression or classification 
    objects. The core functions calculate an in-bag and error bars for random 
    forest objects
    </li>
</ul>
</p>
<p>
Some papers. I will not probably have time to read more than one or two
with the teachings preparation but I should to get more ideas
about students projects.
<ul>
<li><a href="http://statweb.stanford.edu/~ckirby/brad/papers/2013ModelSelection.pdf">Estimation and Accuracy after Model Selection</a>
    </li>
<li><a href="http://jmlr.csail.mit.edu/papers/volume15/wager14a/wager14a.pdf">Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/khaleghi16a/khaleghi16a.pdf">Consistent Algorithms for Clustering Time Series</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/blaser16a/blaser16a.pdf">Random Rotation Ensembles</a>
    </li>
<li><a href="http://www.mlsec.org/harry/">A Tool for Measuring String Similarity</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/15-130/15-130.pdf">Causal Inference through a Witness Protection Program</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/15-346/15-346.pdf">The Statistical Performance of Collaborative Inference</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/14-499/14-499.pdf">Learning Algorithms for Second-Price Auctions with Reserve</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/16-035/16-035.pdf">Spectral Ranking using Seriation</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/15-189/15-189.pdf">Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/15-242/15-242.pdf">The Benefit of Multitask Representation Learning</a>
    </li>
<li><a href="http://www.jmlr.org/papers/volume17/14-316/14-316.pdf">Convex Calibration Dimension for Multiclass Loss Matrices</a>
    </li></ul>
</p>

</body>
</html>
