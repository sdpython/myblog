<?xml version="1.0" encoding="utf-8"?>
<html>
<head> 
<link TYPE="text/css" href="pMenu.css" rel="stylesheet"/>
<link type="text/css" rel="stylesheet" href="documents/graph_blog_20131130.css" />
<link type="text/css" rel="stylesheet" href="javascript/d3.js" />
<title>Lectures et algorithmes un vendredi</title>
<meta content="R, python" name="keywords" />
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
</head>
<body>

<p>
Je commencerai par le plus facile : 
<a href="http://www.lemonde.fr/planete/article/2016/06/10/dans-la-vallee-de-la-bievre-la-crue-et-les-inondations-ont-ete-contenues_4948060_3244.html">Dans la vallée de la Bièvre, la crue et les inondations ont été contenues</a>.
En bref, 18 km de rivière sont régulées par des vannes commandées par un algorithme.
Et cela a fonctionné.
</p>
<p>
Côté programmation, <a href="https://www.tensorflow.org/">TensorFlow</a> commence
à faire des petits. En voici un : 
<a href="http://tflearn.org/getting_started/">tflearn</a> qui propose
une API Python différente pour les fonctionnalités de <i>TensorFlow</i>.
De fil en aiguille, on remonte jusqu'au compte 
<a href="https://github.com/aymericdamien">GitHub</a> de l'auteur
pour tomber sur des 
<a href="https://github.com/aymericdamien/TensorFlow-Examples">exemples avec TensorFlow</a>
et un autre site 
<a href="https://github.com/ujjwalkarn/Machine-Learning-Tutorials#auto">Machine Learning &amp; Deep Learning Tutorials</a>.
C'est sans doute un peu moins poli que celui-ci 
<a href="http://blog.kaggle.com/2016/04/25/free-kaggle-machine-learning-tutorial-for-python/">Free Kaggle Machine Learning Tutorial for Python</a>.
Ensuite,
<a href="http://jmlr.org/papers/v17/15-408.html">CVXPY</a> est un module qui permet de faire
de l'optimisation convexe avec ou sans contrainte.
Pour fermer cette parenthèse, je vous suggère de retourner sur Kaggle
avec le script de la semaine  
<a href="http://blog.kaggle.com/2016/06/07/may-2016-scripts-of-the-week/">May 2016: Scripts of the Week</a>,
<a href="http://blog.kaggle.com/2016/04/04/february-2016-scripts-of-the-week/">February 2016: Scripts of the Week</a>.
</p>
<p>
Pour finir, quelques articles très récents. Le premier pour comprendre
comment fonctionnent les enchères sur internet
<a href="http://jmlr.org/papers/volume17/14-499/14-499.pdf">Learning Algorithms for Second-Price Auctions with Reserve</a>.
Le second pour avoir envie de faire autre chose que de la sélection de variables :
<a href="http://jmlr.org/papers/volume17/15-444/15-444.pdf">Structure-Leveraged Methods in Breast Cancer Risk Prediction</a>.
Le troisième parce que j'ai toujours eu envie d'étudier les sessions des utilisateurs
d'un moteur de recherche d'une façon différente :
<a href="http://jmlr.org/papers/volume17/14-486/14-486.pdf">A Gibbs Sampler for Learning DAGs</a>.
<i>DAG</i> veut dire <i>directed acyclic graph</i>.
Le quatrième parce que je suis curieux : 
<a href="http://jmlr.org/papers/v17/14-441.html">Consistent Distribution-Free K
-Sample and Independence Tests for Univariate Random Variables</a>.
Le cinquième pour le titre : 
<a href="http://jmlr.org/papers/v17/13-589.html">Learning Using Anti-Training with Sacrificial Data"</a>.
Enfin, dans les sujets en vogue, l'apprentissage par renforcement :
<a href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson sampling</a> et
<a href="http://jmlr.org/papers/volume17/14-087/14-087.pdf">An Information-Theoretic Analysis of Thompson Sampling</a>.
</p>


</body>
</html>
